{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1517506c",
   "metadata": {},
   "source": [
    "# ğŸš€ TurboBatch Tutorial: High-Performance Transformer Inference\n",
    "\n",
    "Welcome to the **TurboBatch** tutorial! This notebook will guide you through using TurboBatch to achieve **10x faster** transformer inference through intelligent dynamic batching.\n",
    "\n",
    "## ğŸ“– What You'll Learn:\n",
    "- âš¡ How to set up TurboBatch for your models\n",
    "- ğŸ¯ Basic and advanced usage patterns  \n",
    "- ğŸ“Š Performance comparisons and benchmarks\n",
    "- ğŸ”§ Optimization techniques and best practices\n",
    "- ğŸš€ Real-world examples and use cases\n",
    "\n",
    "## ğŸ¯ Prerequisites:\n",
    "- Basic Python knowledge\n",
    "- Familiarity with Transformers library\n",
    "- Understanding of NLP concepts (helpful but not required)\n",
    "\n",
    "Let's get started! ğŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788817b1",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation and Setup\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe062349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# %pip install torch transformers numpy matplotlib seaborn tqdm turbobatch\n",
    "\n",
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")\n",
    "print(f\"ğŸ Python version: {sys.version}\")\n",
    "print(f\"ğŸ“Š NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TurboBatch (assuming it's in the parent directory or installed via pip)\n",
    "sys.path.append('..')\n",
    "from turbobatch import TurboBatcher, PredictionResult, BatchStats\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"âœ… TurboBatch imported successfully!\")\n",
    "print(\"ğŸš€ Ready to accelerate your transformer inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546ec07",
   "metadata": {},
   "source": [
    "## ğŸƒâ€â™‚ï¸ Quick Start: Your First Dynamic Batch\n",
    "\n",
    "Let's start with a simple sentiment analysis example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"ğŸ“¥ Loaded model: {model_name}\")\n",
    "print(f\"ğŸ·ï¸  Model type: {type(model).__name__}\")\n",
    "print(f\"ğŸ“Š Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TurboBatcher instance\n",
    "batcher = TurboBatcher(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_batch_size=16,\n",
    "    timeout_ms=100,\n",
    "    adaptive_batching=True,\n",
    "    performance_monitoring=True\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ TurboBatcher created with the following configuration:\")\n",
    "print(f\"   ğŸ“¦ Max batch size: {batcher.max_batch_size}\")\n",
    "print(f\"   â±ï¸  Timeout: {batcher.timeout_ms}ms\")\n",
    "print(f\"   ğŸ§  Adaptive batching: {batcher.adaptive_batching}\")\n",
    "print(f\"   ğŸ“ˆ Performance monitoring: {batcher.performance_monitoring}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440afd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some test data\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst thing I've ever bought.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Outstanding quality and excellent customer service!\",\n",
    "    \"Poor build quality, would not recommend.\",\n",
    "    \"Great value for money, very satisfied!\",\n",
    "    \"Disappointing experience overall.\",\n",
    "    \"Perfect! Exactly what I was looking for.\",\n",
    "    \"Could be better but it works fine.\",\n",
    "    \"Incredible performance, highly recommended!\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ Prepared {len(test_texts)} test texts for prediction\")\n",
    "\n",
    "# Quick single prediction\n",
    "print(\"\\nğŸ” Single prediction example:\")\n",
    "start_time = time.time()\n",
    "predictions = batcher.predict(test_texts)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"âœ… Completed {len(predictions)} predictions in {end_time - start_time:.3f} seconds\")\n",
    "print(f\"\ude80 Throughput: {len(predictions) / (end_time - start_time):.2f} predictions/second\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nğŸ“Š Prediction Results:\")\n",
    "for i, (text, pred) in enumerate(zip(test_texts, predictions)):\n",
    "    sentiment = \"POSITIVE ğŸ˜Š\" if pred.label == 1 else \"NEGATIVE ğŸ˜\"\n",
    "    print(f\"{i+1:2d}. {sentiment} ({pred.score:.3f}) | {text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a sentiment analysis API wrapper\n",
    "class SentimentAPI:\n",
    "    \"\"\"Simple API wrapper using TurboBatcher for efficient sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, max_batch_size=32):\n",
    "        self.batcher = TurboBatcher(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_batch_size=max_batch_size,\n",
    "            adaptive_batching=True,\n",
    "            performance_monitoring=True\n",
    "        )\n",
    "        \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze sentiment for a single text\"\"\"\n",
    "        result = self.batcher.predict(text)\n",
    "        return {\n",
    "            'text': text,\n",
    "            'sentiment': 'POSITIVE' if result.label == 1 else 'NEGATIVE',\n",
    "            'confidence': result.score,\n",
    "            'processing_time': result.processing_time\n",
    "        }\n",
    "    \n",
    "    def batch_analyze(self, texts):\n",
    "        \"\"\"Analyze sentiment for multiple texts efficiently\"\"\"\n",
    "        results = self.batcher.predict(texts)\n",
    "        return [\n",
    "            {\n",
    "                'text': text,\n",
    "                'sentiment': 'POSITIVE' if result.label == 1 else 'NEGATIVE',\n",
    "                'confidence': result.score,\n",
    "                'processing_time': result.processing_time\n",
    "            }\n",
    "            for text, result in zip(texts, results)\n",
    "        ]\n",
    "\n",
    "# Create API instance\n",
    "api = SentimentAPI(model, tokenizer)\n",
    "\n",
    "# Test single analysis\n",
    "sample_text = \"This product exceeded my expectations!\"\n",
    "result = api.analyze_sentiment(sample_text)\n",
    "print(\"ğŸ“ Single Analysis Result:\")\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "\n",
    "# Test batch analysis\n",
    "batch_results = api.batch_analyze(test_texts[:5])\n",
    "print(f\"\\nğŸ“Š Batch Analysis Results ({len(batch_results)} texts):\")\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"{i+1}. {result['sentiment']:8s} ({result['confidence']:.3f}) | {result['text'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df448ad0",
   "metadata": {},
   "source": [
    "## ğŸ“Š Performance Comparison: Batched vs Individual Processing\n",
    "\n",
    "Let's see the power of dynamic batching in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(num_samples: int) -> List[str]:\n",
    "    \"\"\"Generate diverse test data for benchmarking\"\"\"\n",
    "    base_texts = [\n",
    "        \"I love this amazing product!\",\n",
    "        \"This is absolutely terrible.\",\n",
    "        \"The weather is quite nice today.\",\n",
    "        \"I'm feeling neutral about this situation.\",\n",
    "        \"Fantastic results! Highly recommended!\",\n",
    "        \"Poor quality, very disappointed.\",\n",
    "        \"It's okay, meets basic requirements.\",\n",
    "        \"Excellent value for money!\",\n",
    "        \"Could be improved but not bad.\",\n",
    "        \"Perfect solution for my needs!\"\n",
    "    ]\n",
    "    \n",
    "    return [base_texts[i % len(base_texts)] + f\" (Sample {i+1})\" for i in range(num_samples)]\n",
    "\n",
    "# Generate test data\n",
    "test_data = generate_test_data(100)\n",
    "print(f\"ğŸ“Š Generated {len(test_data)} test samples for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark individual vs batched processing\n",
    "def benchmark_individual_processing(texts: List[str]):\n",
    "    \"\"\"Benchmark individual processing (one by one)\"\"\"\n",
    "    individual_batcher = DynamicBatcher(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_batch_size=1,  # Force individual processing\n",
    "        timeout_ms=0,\n",
    "        adaptive_batching=False\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions = individual_batcher.predict(texts)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time, predictions\n",
    "\n",
    "def benchmark_batched_processing(texts: List[str], batch_size: int = 16):\n",
    "    \"\"\"Benchmark batched processing\"\"\"\n",
    "    batched_batcher = DynamicBatcher(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_batch_size=batch_size,\n",
    "        timeout_ms=50,\n",
    "        adaptive_batching=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions = batched_batcher.predict(texts)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time, predictions\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"ğŸƒâ€â™‚ï¸ Running individual processing benchmark...\")\n",
    "individual_time, individual_preds = benchmark_individual_processing(test_data)\n",
    "\n",
    "print(\"ğŸš€ Running batched processing benchmark...\")\n",
    "batched_time, batched_preds = benchmark_batched_processing(test_data)\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = individual_time / batched_time\n",
    "\n",
    "print(\"\\nğŸ“Š Benchmark Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Individual Processing: {individual_time:.3f}s\")\n",
    "print(f\"Batched Processing:    {batched_time:.3f}s\")\n",
    "print(f\"Speedup:              {speedup:.2f}x faster! ğŸš€\")\n",
    "print(f\"Throughput Individual: {len(test_data)/individual_time:.2f} samples/sec\")\n",
    "print(f\"Throughput Batched:    {len(test_data)/batched_time:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Processing time comparison\n",
    "methods = ['Individual\\nProcessing', 'Dynamic\\nBatching']\n",
    "times = [individual_time, batched_time]\n",
    "colors = ['#ff7f7f', '#7fbf7f']\n",
    "\n",
    "bars1 = ax1.bar(methods, times, color=colors, alpha=0.8)\n",
    "ax1.set_ylabel('Processing Time (seconds)')\n",
    "ax1.set_title('â±ï¸ Processing Time Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars1, times):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{time_val:.3f}s',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Throughput comparison\n",
    "throughputs = [len(test_data)/individual_time, len(test_data)/batched_time]\n",
    "bars2 = ax2.bar(methods, throughputs, color=colors, alpha=0.8)\n",
    "ax2.set_ylabel('Throughput (samples/second)')\n",
    "ax2.set_title('ğŸš€ Throughput Comparison')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, throughput in zip(bars2, throughputs):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{throughput:.1f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ‰ Dynamic Batching is {speedup:.2f}x faster than individual processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9c71a",
   "metadata": {},
   "source": [
    "## ğŸ¯ Batch Size Optimization\n",
    "\n",
    "Let's find the optimal batch size for maximum performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "performance_results = {}\n",
    "\n",
    "print(\"ğŸ” Testing different batch sizes...\")\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"  Testing batch size: {batch_size}\")\n",
    "    \n",
    "    # Create batcher with specific batch size\n",
    "    test_batcher = TurboBatcher(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_batch_size=batch_size,\n",
    "        timeout_ms=50,\n",
    "        adaptive_batching=False  # Fixed batch size for this test\n",
    "    )\n",
    "    \n",
    "    # Warm up\n",
    "    _ = test_batcher.predict(test_data[:5])\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    predictions = test_batcher.predict(test_data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    processing_time = end_time - start_time\n",
    "    throughput = len(test_data) / processing_time\n",
    "    \n",
    "    performance_results[batch_size] = {\n",
    "        'time': processing_time,\n",
    "        'throughput': throughput\n",
    "    }\n",
    "    \n",
    "    print(f\"    â±ï¸  Time: {processing_time:.3f}s | ğŸš€ Throughput: {throughput:.2f} samples/sec\")\n",
    "\n",
    "print(\"\\nâœ… Batch size optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch size optimization results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "batch_sizes_list = list(performance_results.keys())\n",
    "times = [performance_results[bs]['time'] for bs in batch_sizes_list]\n",
    "throughputs = [performance_results[bs]['throughput'] for bs in batch_sizes_list]\n",
    "\n",
    "# Processing time vs batch size\n",
    "ax1.plot(batch_sizes_list, times, marker='o', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Processing Time (seconds)')\n",
    "ax1.set_title('â±ï¸ Processing Time vs Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log', base=2)\n",
    "\n",
    "# Throughput vs batch size\n",
    "ax2.plot(batch_sizes_list, throughputs, marker='s', linewidth=2, markersize=8, color='#27ae60')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Throughput (samples/second)')\n",
    "ax2.set_title('ğŸš€ Throughput vs Batch Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "# Find optimal batch size\n",
    "optimal_batch_size = max(performance_results.keys(), key=lambda x: performance_results[x]['throughput'])\n",
    "optimal_throughput = performance_results[optimal_batch_size]['throughput']\n",
    "\n",
    "# Mark optimal point\n",
    "ax2.scatter([optimal_batch_size], [optimal_throughput], \n",
    "           color='red', s=100, zorder=5, label=f'Optimal: {optimal_batch_size}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ Optimal batch size: {optimal_batch_size}\")\n",
    "print(f\"ğŸš€ Maximum throughput: {optimal_throughput:.2f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b02f2",
   "metadata": {},
   "source": [
    "## ğŸ§  Adaptive Batching in Action\n",
    "\n",
    "Watch how TurboBatcher automatically adapts to different workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adaptive batcher\n",
    "adaptive_batcher = TurboBatcher(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_batch_size=32,\n",
    "    timeout_ms=100,\n",
    "    adaptive_batching=True,\n",
    "    performance_monitoring=True\n",
    ")\n",
    "\n",
    "# Simulate different workload scenarios\n",
    "scenarios = [\n",
    "    (\"ğŸŒ Light Load\", generate_test_data(5)),\n",
    "    (\"ğŸ“ˆ Medium Load\", generate_test_data(25)),\n",
    "    (\"ğŸš€ Heavy Load\", generate_test_data(100)),\n",
    "    (\"ğŸ’¥ Burst Load\", generate_test_data(200)),\n",
    "]\n",
    "\n",
    "adaptive_results = []\n",
    "\n",
    "print(\"ğŸ§  Testing Adaptive Batching with different workloads...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for scenario_name, test_data in scenarios:\n",
    "    print(f\"\\n{scenario_name} - {len(test_data)} samples:\")\n",
    "    \n",
    "    # Reset performance stats\n",
    "    adaptive_batcher._reset_stats() if hasattr(adaptive_batcher, '_reset_stats') else None\n",
    "    \n",
    "    # Process with timing\n",
    "    start_time = time.time()\n",
    "    predictions = adaptive_batcher.predict(test_data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Get performance stats\n",
    "    stats = adaptive_batcher.get_performance_stats()\n",
    "    \n",
    "    processing_time = end_time - start_time\n",
    "    throughput = len(test_data) / processing_time\n",
    "    \n",
    "    result = {\n",
    "        'scenario': scenario_name,\n",
    "        'samples': len(test_data),\n",
    "        'time': processing_time,\n",
    "        'throughput': throughput,\n",
    "        'avg_batch_size': stats.get('avg_batch_size', 'N/A'),\n",
    "        'total_batches': stats.get('total_batches', 'N/A')\n",
    "    }\n",
    "    \n",
    "    adaptive_results.append(result)\n",
    "    \n",
    "    print(f\"  â±ï¸  Time: {processing_time:.3f}s\")\n",
    "    print(f\"  ğŸš€ Throughput: {throughput:.2f} samples/sec\")\n",
    "    print(f\"  \udce6 Avg batch size: {result['avg_batch_size']}\")\n",
    "    print(f\"  \udd22 Total batches: {result['total_batches']}\")\n",
    "\n",
    "print(f\"\\nâœ… Adaptive batching test completed!\")\n",
    "print(\"ğŸ“Š Notice how TurboBatcher adapts batch sizes based on workload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adaptive batching results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸ§  Adaptive Batching Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "scenarios_names = [r['scenario'] for r in adaptive_results]\n",
    "sample_counts = [r['samples'] for r in adaptive_results]\n",
    "throughputs = [r['throughput'] for r in adaptive_results]\n",
    "times = [r['time'] for r in adaptive_results]\n",
    "\n",
    "# Throughput by scenario\n",
    "bars1 = ax1.bar(range(len(scenarios_names)), throughputs, \n",
    "                color=['#3498db', '#e67e22', '#e74c3c', '#9b59b6'], alpha=0.8)\n",
    "ax1.set_xticks(range(len(scenarios_names)))\n",
    "ax1.set_xticklabels([s.split()[1] for s in scenarios_names], rotation=45)\n",
    "ax1.set_ylabel('Throughput (samples/sec)')\n",
    "ax1.set_title('ğŸš€ Throughput by Workload')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing time by scenario\n",
    "bars2 = ax2.bar(range(len(scenarios_names)), times,\n",
    "                color=['#3498db', '#e67e22', '#e74c3c', '#9b59b6'], alpha=0.8)\n",
    "ax2.set_xticks(range(len(scenarios_names)))\n",
    "ax2.set_xticklabels([s.split()[1] for s in scenarios_names], rotation=45)\n",
    "ax2.set_ylabel('Processing Time (seconds)')\n",
    "ax2.set_title('â±ï¸ Processing Time by Workload')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency (throughput per sample)\n",
    "efficiency = [t/s for t, s in zip(throughputs, sample_counts)]\n",
    "bars3 = ax3.bar(range(len(scenarios_names)), efficiency,\n",
    "                color=['#3498db', '#e67e22', '#e74c3c', '#9b59b6'], alpha=0.8)\n",
    "ax3.set_xticks(range(len(scenarios_names)))\n",
    "ax3.set_xticklabels([s.split()[1] for s in scenarios_names], rotation=45)\n",
    "ax3.set_ylabel('Efficiency (throughput/sample)')\n",
    "ax3.set_title('âš¡ Processing Efficiency')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Scalability chart\n",
    "ax4.scatter(sample_counts, throughputs, s=100, \n",
    "           c=['#3498db', '#e67e22', '#e74c3c', '#9b59b6'], alpha=0.8)\n",
    "ax4.set_xlabel('Number of Samples')\n",
    "ax4.set_ylabel('Throughput (samples/sec)')\n",
    "ax4.set_title('ğŸ“ˆ Scalability Analysis')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(sample_counts, throughputs, 1)\n",
    "p = np.poly1d(z)\n",
    "ax4.plot(sample_counts, p(sample_counts), \"--\", alpha=0.7, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Adaptive batching automatically optimizes performance based on workload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcdea6",
   "metadata": {},
   "source": [
    "## ğŸ¯ Real-World Use Case: Sentiment Analysis API\n",
    "\n",
    "Let's simulate a real-world scenario where you're building a sentiment analysis API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "class SentimentAPI:\n",
    "    \"\"\"Simulated sentiment analysis API using TurboBatcher\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batcher = TurboBatcher(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_batch_size=32,\n",
    "            timeout_ms=50,\n",
    "            adaptive_batching=True,\n",
    "            performance_monitoring=True\n",
    "        )\n",
    "        self.request_count = 0\n",
    "        self.total_processing_time = 0\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> dict:\n",
    "        \"\"\"Analyze sentiment of a single text\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use TurboBatcher for efficient processing\n",
    "        result = self.batcher.predict([text])[0]\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        self.request_count += 1\n",
    "        self.total_processing_time += (end_time - start_time)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'sentiment': 'POSITIVE' if result.label == 1 else 'NEGATIVE',\n",
    "            'confidence': result.score,\n",
    "            'processing_time': end_time - start_time\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get API performance statistics\"\"\"\n",
    "        avg_response_time = self.total_processing_time / self.request_count if self.request_count > 0 else 0\n",
    "        batcher_stats = self.batcher.get_performance_stats()\n",
    "        \n",
    "        return {\n",
    "            'total_requests': self.request_count,\n",
    "            'avg_response_time': avg_response_time,\n",
    "            'total_processing_time': self.total_processing_time,\n",
    "            'batcher_stats': batcher_stats\n",
    "        }\n",
    "\n",
    "# Create API instance\n",
    "api = SentimentAPI()\n",
    "print(\"ğŸŒ Sentiment Analysis API created with TurboBatcher backend!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff075ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate realistic API requests\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! I love it!\",\n",
    "    \"Worst purchase ever. Complete waste of money.\",\n",
    "    \"Good quality, fast delivery. Satisfied with the purchase.\",\n",
    "    \"The item arrived damaged and customer service was unhelpful.\",\n",
    "    \"Excellent value for money. Highly recommend!\",\n",
    "    \"Product doesn't match the description. Very disappointed.\",\n",
    "    \"Fast shipping, great packaging. Product works perfectly.\",\n",
    "    \"Poor quality materials. Broke after just one week.\",\n",
    "    \"Outstanding customer service and premium quality product.\",\n",
    "    \"Overpriced for what you get. Not worth the money.\"\n",
    "]\n",
    "\n",
    "def simulate_user_request():\n",
    "    \"\"\"Simulate a user making an API request\"\"\"\n",
    "    review = random.choice(sample_reviews)\n",
    "    # Add some randomness to make it more realistic\n",
    "    review += f\" (User {random.randint(1, 1000)})\"\n",
    "    \n",
    "    return api.analyze_sentiment(review)\n",
    "\n",
    "# Simulate concurrent API requests\n",
    "print(\"ğŸš€ Simulating 100 concurrent API requests...\")\n",
    "start_simulation = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(simulate_user_request) for _ in range(100)]\n",
    "    results = [future.result() for future in futures]\n",
    "\n",
    "end_simulation = time.time()\n",
    "simulation_time = end_simulation - start_simulation\n",
    "\n",
    "print(f\"âœ… Completed 100 API requests in {simulation_time:.3f} seconds\")\n",
    "print(f\"ğŸš€ API Throughput: {100/simulation_time:.2f} requests/second\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nğŸ“Š Sample API Responses:\")\n",
    "print(\"-\" * 100)\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f\"{i+1}. {result['sentiment']:8s} ({result['confidence']:.3f}) | {result['text'][:60]}...\")\n",
    "    print(f\"   â±ï¸  Response time: {result['processing_time']*1000:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "# Get API statistics\n",
    "api_stats = api.get_stats()\n",
    "print(\"ğŸ“ˆ Final API Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"ğŸ”¢ Total requests: {api_stats['total_requests']}\")\n",
    "print(f\"â±ï¸  Average response time: {api_stats['avg_response_time']*1000:.2f}ms\")\n",
    "print(f\"ğŸš€ Total processing time: {api_stats['total_processing_time']:.3f}s\")\n",
    "print(\"\\nğŸ“Š TurboBatcher Statistics:\")\n",
    "batcher_stats = api_stats['batcher_stats']\n",
    "for key, value in batcher_stats.items():\n",
    "    if key != 'device':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ TurboBatch handled {api_stats['total_requests']} requests efficiently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dfec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze API performance\n",
    "api_stats = api.get_stats()\n",
    "\n",
    "print(\"ğŸ“ˆ API Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Requests Processed: {api_stats['total_requests']}\")\n",
    "print(f\"Average Response Time: {api_stats['avg_response_time']*1000:.2f}ms\")\n",
    "print(f\"Total Processing Time: {api_stats['total_processing_time']:.3f}s\")\n",
    "print(f\"API Efficiency: {(api_stats['total_processing_time']/simulation_time)*100:.1f}% CPU utilization\")\n",
    "\n",
    "print(\"\\nğŸ§  DynamicBatcher Statistics:\")\n",
    "print(\"-\" * 30)\n",
    "batcher_stats = api_stats['batcher_stats']\n",
    "for key, value in batcher_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Visualize response times\n",
    "response_times = [r['processing_time'] * 1000 for r in results]  # Convert to ms\n",
    "sentiments = [r['sentiment'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Response time distribution\n",
    "ax1.hist(response_times, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(np.mean(response_times), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(response_times):.2f}ms')\n",
    "ax1.set_xlabel('Response Time (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('â±ï¸ API Response Time Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = {}\n",
    "for sentiment in sentiments:\n",
    "    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
    "\n",
    "colors = ['#e74c3c' if s == 'NEGATIVE' else '#27ae60' for s in sentiment_counts.keys()]\n",
    "ax2.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), \n",
    "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax2.set_title('ğŸ˜Š Sentiment Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ‰ API successfully processed {len(results)} requests with an average response time of {np.mean(response_times):.2f}ms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35998e1",
   "metadata": {},
   "source": [
    "## ğŸ”§ Advanced Configuration and Tips\n",
    "\n",
    "Here are some advanced tips for getting the most out of TurboBatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced configuration examples\n",
    "print(\"ğŸ”§ Advanced TurboBatch Configurations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        'name': 'High Throughput',\n",
    "        'config': {\n",
    "            'max_batch_size': 64,\n",
    "            'timeout_ms': 200,\n",
    "            'adaptive_batching': True,\n",
    "            'performance_monitoring': True\n",
    "        },\n",
    "        'use_case': 'Batch processing, offline analysis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low Latency',\n",
    "        'config': {\n",
    "            'max_batch_size': 8,\n",
    "            'timeout_ms': 10,\n",
    "            'adaptive_batching': True,\n",
    "            'performance_monitoring': False\n",
    "        },\n",
    "        'use_case': 'Real-time APIs, interactive applications'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Memory Optimized',\n",
    "        'config': {\n",
    "            'max_batch_size': 16,\n",
    "            'timeout_ms': 100,\n",
    "            'adaptive_batching': True,\n",
    "            'performance_monitoring': True\n",
    "        },\n",
    "        'use_case': 'Limited GPU memory, large models'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Balanced',\n",
    "        'config': {\n",
    "            'max_batch_size': 32,\n",
    "            'timeout_ms': 50,\n",
    "            'adaptive_batching': True,\n",
    "            'performance_monitoring': True\n",
    "        },\n",
    "        'use_case': 'General purpose, most applications'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, config_info in enumerate(configurations, 1):\n",
    "    print(f\"\\n{i}. {config_info['name']} Configuration:\")\n",
    "    print(f\"   Use Case: {config_info['use_case']}\")\n",
    "    print(\"   Settings:\")\n",
    "    for key, value in config_info['config'].items():\n",
    "        print(f\"     {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Pro Tips:\")\n",
    "tips = [\n",
    "    \"ğŸš€ Use larger batch sizes for offline processing\",\n",
    "    \"âš¡ Use smaller timeouts for real-time applications\", \n",
    "    \"ğŸ§  Enable adaptive batching for variable workloads\",\n",
    "    \"ğŸ“Š Enable performance monitoring for optimization insights\",\n",
    "    \"ğŸ’¾ Monitor GPU memory usage when increasing batch sizes\",\n",
    "    \"ğŸ”„ Profile your specific use case to find optimal settings\"\n",
    "]\n",
    "\n",
    "for tip in tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "print(\"\\nğŸ”¥ Example: Creating a high-throughput configuration\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example of creating a high-throughput batcher\n",
    "high_throughput_batcher = TurboBatcher(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_batch_size=64,\n",
    "    timeout_ms=200,\n",
    "    adaptive_batching=True,\n",
    "    performance_monitoring=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ… High-throughput TurboBatcher created!\")\n",
    "print(f\"ğŸ“¦ Max batch size: {high_throughput_batcher.max_batch_size}\")\n",
    "print(f\"â±ï¸  Timeout: {high_throughput_batcher.timeout_ms}ms\")\n",
    "print(f\"ğŸ§  Adaptive batching: {high_throughput_batcher.adaptive_batching}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c68d5",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "Congratulations! You've successfully learned how to use **TurboBatch** to dramatically improve your transformer inference performance.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "âœ… **Performance Boost**: TurboBatch can provide 5-10x speedup over individual processing\n",
    "\n",
    "âœ… **Easy Integration**: Works seamlessly with any HuggingFace transformer model\n",
    "\n",
    "âœ… **Adaptive Intelligence**: Automatically optimizes batch sizes based on workload\n",
    "\n",
    "âœ… **Production Ready**: Built-in monitoring and performance analytics\n",
    "\n",
    "âœ… **Memory Efficient**: Smart memory management prevents OOM errors\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. ğŸ”„ **Try it with your own models** - TurboBatch works with any transformer!\n",
    "2. ğŸ“Š **Experiment with configurations** - Find the optimal settings for your use case\n",
    "3. ğŸš€ **Deploy to production** - Scale your ML applications with confidence\n",
    "4. ğŸ“ˆ **Monitor performance** - Use built-in analytics to optimize further\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- ğŸ“– **GitHub Repository**: [github.com/Shayanthn/turbobatch](https://github.com/Shayanthn/turbobatch)\n",
    "- ğŸ“š **Documentation**: Check the README for detailed API reference\n",
    "- ğŸ’¬ **Discussions**: Join the community for questions and ideas\n",
    "- ğŸ› **Issues**: Report bugs or request features\n",
    "\n",
    "### Thank You! ğŸ™\n",
    "\n",
    "Thank you for exploring TurboBatch! If this helped accelerate your projects, please:\n",
    "- â­ **Star the repository** on GitHub\n",
    "- ğŸ“¢ **Share with colleagues** who work with transformers\n",
    "- ğŸ’¡ **Contribute ideas** for improvements\n",
    "\n",
    "**Happy Accelerating! ğŸš€**\n",
    "\n",
    "---\n",
    "\n",
    "*Created by [Shayan Taherkhani](https://shayantaherkhani.ir) with â¤ï¸*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
